# ============================== Filebeat inputs ===============================
filebeat.inputs:
  #- type: filestream
  #id: my-csv-stream-id # A unique ID for this input stream
  #enabled: true

  # Paths that should be crawled and fetched. Glob based paths.
  #paths:
  #  - /var/log/input_data/corrupted_logs.csv # Path INSIDE the container where the CSV is mounted

  # Ensure Filebeat doesn't try to parse the lines as JSON
  #parsers: []

  # Settings for reading a static file. Filebeat will read the file on startup.
  # If the file is replaced, it will be rescanned after check_interval.
  #scan.on_startup: true
  #prospector.scanner.check_interval: 30s
  #close.on_state_change.inactive: 1m # Close file handle if inactive for 1 minute

# ============================== Filebeat outputs ==============================

# Disable Elasticsearch output directly from Filebeat
output.elasticsearch:
  enabled: false

# Configure Logstash output
output.logstash:
  # The Logstash hosts
  hosts: ["logstash:5044"] # Address of the Logstash container

# ============================== Logging ======================================
# Sets log level. The default log level is info.
# Available log levels are: error, warning, info, debug
logging.level: info

# Log to rotating files
logging.to_files: true
logging.files:
  path: /usr/share/filebeat/logs
  name: filebeat
  keepfiles: 7
  permissions: 0644

# ============================== Processors =====================================
# Optional processors to add metadata
processors:
  - add_host_metadata: ~
  - add_cloud_metadata: ~
  - add_docker_metadata: ~
