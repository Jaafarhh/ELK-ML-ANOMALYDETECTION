# This pipeline receives log lines via TCP, parses them as CSV,
# processes the fields, and sends them to Elasticsearch.

input {
  tcp {
    port => 5045 # Port the Python script will send data to
    codec => line { charset => "UTF-8" } # Treat each line as a separate event
  }
}

filter {
  # Use Grok to parse the line.
  # The IdProcess pattern ([^,]*) now allows zero or more non-comma characters.
  grok {
    match => { "message" => "^%{TIMESTAMP_ISO8601:Date},(?<Hostname>[^,]+),(?<Process>[^,]+),(?<IdProcess>[^,]*),%{GREEDYDATA:Message}$" }
    overwrite => [ "message" ]
  }

  # Check if Grok failed (e.g., line was completely malformed like ",,,,")
  if "_grokparsefailure" in [tags] {
    # Tag the event for investigation
    mutate {
      add_tag => ["_grokparsefailure_investigate"]
    }
  } else {
    # --- Post-processing for successful grok matches ---

    # If IdProcess was captured as an empty string, replace it with "N/A"
    if ![IdProcess] or [IdProcess] == "" {
      mutate {
        replace => { "IdProcess" => "N/A" }
      }
    }

    # --- Call ML Inference Service ---
    http {
      url => "http://ml_inference:5000/predict" # URL of the service within Docker network
      verb => "POST"
      headers => {
        "Content-Type" => "application/json"
      }
      # Send relevant fields needed by the model
      body => {
        "Hostname" => "%{Hostname}"
        "Process" => "%{Process}"
        "Message" => "%{Message}"
      }
      body_format => "json"
      # Target field to store the parsed JSON response from the ML service
      target_body => "[@metadata][ml_response]"
    }

    # Check if the HTTP request itself failed
    if "_httprequestfailure" in [tags] {
        mutate {
            add_tag => ["_ml_service_call_failed"] # Add our custom tag if the standard failure tag exists
        }
    } else {
        # HTTP request succeeded, now check the response from the ML service
        if ![@metadata][ml_response][error] and [@metadata][ml_response][anomaly_prediction] {
           # Prediction field exists and no error reported by the service
           mutate {
             add_field => { "ml_anomaly_prediction" => "%{[@metadata][ml_response][anomaly_prediction]}" }
             convert => { "ml_anomaly_prediction" => "integer" }
             add_tag => ["_ml_prediction_ok"]
           }
        } else {
           # ML call succeeded but service returned an error or unexpected response
           mutate {
             add_tag => ["_ml_prediction_failed"]
             # Optionally add the error message from the service to the event
             # add_field => { "ml_error" => "%{[@metadata][ml_response][error]}" }
           }
        }
    } # End of check for _httprequestfailure

    # --- Traditional Rule Implementation ---

    # Rule 1: gnome-shell extension already installed (from only_anomalies.csv)
    if [Process] == "gnome-shell" and "already installed" in [Message] and "will not be loaded" in [Message] {
      mutate {
        add_tag => ["traditional_alert", "gnome_extension_conflict"]
      }
    }

    # Rule 2: gnome-shell disposed object error (from only_anomalies.csv)
    else if [Process] == "gnome-shell" and "has been already disposed" in [Message] {
      mutate {
        add_tag => ["traditional_alert", "gnome_disposed_object"]
      }
    }

    # Rule 3: dbus-daemon activating tracker-miner-* (representative from only_anomalies.csv)
    else if [Process] == "dbus-daemon" and "Activating via systemd" in [Message] and "tracker-miner" in [Message] {
      mutate {
        add_tag => ["traditional_alert", "dbus_tracker_activation"]
      }
    }

    # Rule 4: gnome-shell assertion failure (from corrupted_logs.csv)
    else if [Process] == "gnome-shell" and "meta_window_set_stack_position_no_sync" in [Message] and "assertion" in [Message] {
      mutate {
        add_tag => ["traditional_alert", "gnome_assertion_stack_position"]
      }
    }

    # Rule 5: Broken pipe errors (from corrupted_logs.csv)
    # Covers evolution-alarm, kdeconnectd, xdg-desktop-por
    else if "Error reading events from display: Relais brisÃ© (pipe)" in [Message] {
       mutate {
         add_tag => ["traditional_alert", "broken_pipe_error"]
       }
    }

    # Rule 6: udisksd failed to load module (from corrupted_logs.csv)
    else if [Process] == "udisksd" and "failed to load module" in [Message] {
       mutate {
         add_tag => ["traditional_alert", "udisksd_module_load_failure"]
       }
    }

    # Rule 7: wireplumber libcamera missing (from corrupted_logs.csv)
    else if [Process] == "wireplumber" and "libcamera SPA missing or broken" in [Message] {
       mutate {
         add_tag => ["traditional_alert", "wireplumber_libcamera_missing"]
       }
    }

    # Rule 8: tracker-miner unable to get XDG path (from corrupted_logs.csv)
    else if [Process] == "tracker-miner-f" and "Unable to get XDG user directory path" in [Message] {
       mutate {
         add_tag => ["traditional_alert", "tracker_xdg_path_error"]
       }
    }

    # Rule 9: General assertion failures (common pattern) - CORRECTED STRUCTURE
    else if "assertion failed" in [Message] {
        # Check if already tagged by a more specific rule BEFORE mutating
        if "traditional_alert" not in [tags] {
            mutate {
                add_tag => ["traditional_alert", "general_assertion_failure"]
            }
        }
    }

    # Rule 10: General 'failed' keyword (use cautiously, might be noisy)
    # else if "failed" in [Message] {
    #     if "traditional_alert" not in [tags] {
    #         mutate {
    #             add_tag => ["traditional_alert", "general_failed_keyword"]
    #         }
    #     }
    # }

    # --- End of Traditional Rules ---

    # Perform cleanup and type conversion
    mutate {
      # Strip leading/trailing whitespace and quotes from the captured Message
      strip => ["Message"]
      gsub => [
        "Message", "^\"|\"$", "" # Remove leading/trailing double quotes
      ]

      # Remove fields we don't need
      # Keep 'host' from tcp input temporarily
      remove_field => ["message", "log", "agent", "ecs", "input", "Date", "@version", "event", "[@metadata][ml_response]"]

      # Ensure IdProcess is treated as string
      convert => { "IdProcess" => "string" }

      # Rename fields to be more ECS compliant
      rename => {
          "Hostname" => "host.name"
          "Process" => "process.name"
          "IdProcess" => "process.pid_string"
      }
      # Add success tag
      add_tag => ["linux_log_simulated_grok_ok"]
    }

    # If the process name contains a path, extract just the executable name
    if [process.name] and "/" in [process.name] {
      grok {
        match => { "process.name" => "(?:.*\/)?(?<process.executable>[^/]+)$" }
        overwrite => [ "process.executable" ]
      }
    }

    # Remove the 'host' field added by the TCP input if it's not the same as host.name
    if [host] and [host][name] != [host.name] {
        mutate { remove_field => ["host"] }
    }
  } # <-- End of else block for successful grok
} # <-- End of filter block

output {
  # Send the processed data to Elasticsearch
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "linux-logs-%{+YYYY.MM.dd}"
  }

  # (Optional) Print processed events to Logstash console for debugging
  # stdout { codec => rubydebug }
}