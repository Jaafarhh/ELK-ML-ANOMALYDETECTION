# This pipeline receives log lines via TCP, parses them as CSV,
# processes the fields, and sends them to Elasticsearch.

input {
  tcp {
    port => 5045 # Port the Python script will send data to
    codec => line { charset => "UTF-8" } # Treat each line as a separate event
  }
}

filter {
  # Use Grok to parse the line.
  # The IdProcess pattern ([^,]*) now allows zero or more non-comma characters.
  grok {
    match => { "message" => "^%{TIMESTAMP_ISO8601:Date},(?<Hostname>[^,]+),(?<Process>[^,]+),(?<IdProcess>[^,]*),%{GREEDYDATA:Message}$" }
    overwrite => [ "message" ]
  }

  # Check if Grok failed (e.g., line was completely malformed like ",,,,")
  if "_grokparsefailure" in [tags] {
    # Tag the event for investigation
    mutate {
      add_tag => ["_grokparsefailure_investigate"]
    }
  } else {
    # --- Post-processing for successful grok matches ---

    # If IdProcess was captured as an empty string, replace it with "N/A"
    if ![IdProcess] or [IdProcess] == "" {
      mutate {
        replace => { "IdProcess" => "N/A" }
      }
    }

    # --- Call ML Inference Service ---
    http {
      url => "http://ml_inference:5000/predict" # URL of the service within Docker network
      verb => "POST"
      headers => {
        "Content-Type" => "application/json"
      }
      # Send relevant fields needed by the model
      body => {
        "Hostname" => "%{Hostname}"
        "Process" => "%{Process}"
        "Message" => "%{Message}"
      }
      body_format => "json"
      # Target field to store the parsed JSON response from the ML service
      target_body => "[@metadata][ml_response]"
    }

    # Check if the HTTP request itself failed
    if "_httprequestfailure" in [tags] {
        mutate {
            add_tag => ["_ml_service_call_failed"] # Add our custom tag if the standard failure tag exists
        }
    } else {
        # HTTP request succeeded, now check the response from the ML service
        if ![@metadata][ml_response][error] and [@metadata][ml_response][anomaly_prediction] {
           # Prediction field exists and no error reported by the service
           mutate {
             add_field => { "ml_anomaly_prediction" => "%{[@metadata][ml_response][anomaly_prediction]}" }
             convert => { "ml_anomaly_prediction" => "integer" }
             add_tag => ["_ml_prediction_ok"]
           }
        } else {
           # ML call succeeded but service returned an error or unexpected response
           mutate {
             add_tag => ["_ml_prediction_failed"]
             # Optionally add the error message from the service to the event
             # add_field => { "ml_error" => "%{[@metadata][ml_response][error]}" }
           }
        }
    } # End of check for _httprequestfailure

    # Perform cleanup and type conversion
    mutate {
      # Strip leading/trailing whitespace and quotes from the captured Message
      strip => ["Message"]
      gsub => [
        "Message", "^\"|\"$", "" # Remove leading/trailing double quotes
      ]

      # Remove fields we don't need
      # Keep 'host' from tcp input temporarily
      remove_field => ["message", "log", "agent", "ecs", "input", "Date", "@version", "event", "[@metadata][ml_response]"]

      # Ensure IdProcess is treated as string
      convert => { "IdProcess" => "string" }

      # Rename fields to be more ECS compliant
      rename => {
          "Hostname" => "host.name"
          "Process" => "process.name"
          "IdProcess" => "process.pid_string"
      }
      # Add success tag
      add_tag => ["linux_log_simulated_grok_ok"]
    }

    # If the process name contains a path, extract just the executable name
    if [process.name] and "/" in [process.name] {
      grok {
        match => { "process.name" => "(?:.*\/)?(?<process.executable>[^/]+)$" }
        overwrite => [ "process.executable" ]
      }
    }

    # Remove the 'host' field added by the TCP input if it's not the same as host.name
    if [host] and [host][name] != [host.name] {
        mutate { remove_field => ["host"] }
    }
  } # <-- End of else block for successful grok
} # <-- End of filter block

output {
  # Send the processed data to Elasticsearch
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "linux-logs-%{+YYYY.MM.dd}"
  }

  # (Optional) Print processed events to Logstash console for debugging
  # stdout { codec => rubydebug }
}