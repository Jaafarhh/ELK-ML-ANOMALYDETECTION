# This pipeline receives log lines from Filebeat, parses them as CSV,
# processes the fields, and sends them to Elasticsearch.

input {
  beats {
    port => 5044 # The port Filebeat will send data to
  }
}

filter {
  # Parse the incoming message (which is a line from the CSV)
  csv {
    separator => ","
    columns => ["Date", "Hostname", "Process", "IdProcess", "Message"]
    skip_header => true # Skip the first line (header) of the CSV file
    quote_char => '"'   # Add this line to handle quoted fields
  }

  # Perform some cleanup and type conversion
  mutate {
    # Remove the original raw CSV line field sent by Filebeat
    # Also remove the original Date field as we are using current time
    remove_field => ["message", "log", "agent", "ecs", "input", "host", "Date"]

    # Ensure IdProcess is treated as a string, especially for "N/A" values
    convert => {
      "IdProcess" => "string"
    }

    # Rename fields to be more ECS (Elastic Common Schema) compliant (optional but good practice)
    rename => {
        "Hostname" => "host.name"
        "Process" => "process.name"
        "IdProcess" => "process.pid_string"
    }
    # Add a tag to identify these logs
    add_tag => ["linux_csv_log"]
  }

  # If the process name contains a path, extract just the executable name (optional)
  if [process.name] and "/" in [process.name] {
    grok {
      match => { "process.name" => "(?:.*\/)?(?<process.executable>[^/]+)$" }
      overwrite => [ "process.executable" ]
    }
  }
}

output {
  # Send the processed data to Elasticsearch
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "linux-logs-%{+YYYY.MM.dd}"
    # user => "elastic" # Add username/password if security were enabled
    # password => "changeme"
  }

  # (Optional) Print processed events to Logstash console for debugging
  # stdout { codec => rubydebug }
}