version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.13.2 # Use a specific recent version
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms1g -Xmx1g # Allocate 1GB RAM; adjust if needed
      - xpack.security.enabled=false # Disable security for easier local dev (NOT for production)
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data # Persist ES data
    networks:
      - elk

  logstash:
    image: docker.elastic.co/logstash/logstash:8.13.2
    container_name: logstash
    volumes:
      # Mount the pipeline configuration file
      - ./logstash/pipeline/logstash.conf:/usr/share/logstash/pipeline/logstash.conf:ro
    ports:
      - "5045:5045" # Expose the new TCP input port
    depends_on:
      - elasticsearch
      - ml_inference # Add dependency: Logstash needs ml_inference to be ready
    networks:
      - elk
    environment:
      - LS_JAVA_OPTS=-Xms1g -Xmx1g # Increase to 1GB RAM

  kibana:
    image: docker.elastic.co/kibana/kibana:8.13.2
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200 # Connect Kibana to ES container
      - XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY=a56kcb8927sbdj462kskf83hdue82hdk
      - LOGGING_VERBOSE=true
    depends_on:
      - elasticsearch
    networks:
      - elk

  filebeat:
    image: docker.elastic.co/beats/filebeat:8.13.2
    container_name: filebeat
    user: root # Needed to access mounted volumes potentially owned by host user
    volumes:
      # Mount the Filebeat configuration file
      - ./filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      # Mount the directory containing the CSV file to be read by Filebeat
      - ./logs_processing_and_predicting:/var/log/input_data:ro
      # Mount Filebeat's data directory for registry persistence
      - filebeat_data:/usr/share/filebeat/data
    depends_on:
      - logstash # Keep dependency in case it sends other data later
    networks:
      - elk
    command: ["--strict.perms=false"] # May be needed depending on volume permissions

  log_simulator:
    build:
      context: ./simulator # Path to the directory containing Dockerfile and script
      dockerfile: Dockerfile
    container_name: log_simulator
    volumes:
      # Mount the directory containing the CSV file to be read by the script
      - ./logs_processing_and_predicting:/data:ro
    depends_on:
      - logstash # Simulator sends to Logstash
    networks:
      - elk
    restart: 'no' # Use restart: 'no' so it runs once and stops, or 'on-failure' if you want it to retry

  ml_inference:
    build:
      context: ./ml_inference # Path to the directory with Dockerfile
      dockerfile: Dockerfile
    container_name: ml_inference
    networks:
      - elk
    restart: unless-stopped

networks:
  elk:
    driver: bridge

volumes:
  elasticsearch_data:
    driver: local
  filebeat_data:
    driver: local
